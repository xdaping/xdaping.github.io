---
layout: post
category: "cv"
title: "glow"
tags: [glow, ]
---

目录

<!-- TOC -->


<!-- /TOC -->

参考[换脸效果媲美GAN！一文解析OpenAI最新流生成模型「Glow」](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247490358&idx=1&sn=b4b5d6014bdd365456d500537ba5bcad&chksm=96e9c4b6a19e4da08710a55935dc2e15b00838d5395fdf2a424c50dedff9af7fa11441741b9d&mpshare=1&scene=1&srcid=0713uWBQxj4Q5wCZwuphDYZN&pass_ticket=qgYs5vOKlc87Cj4B5uTln9ELDfWQJnTqwJO%2B5ipNoI6K7VStQ9djW9PXdfzSwMD3#rd)

[Glow: Generative Flow with Invertible 1x1 Convolutions](https://arxiv.org/abs/1807.03039)

虽然基于流的生成模型在 2014 年就已经提出来了，但是一直没有得到重视。Glow 的作者在之前已经在基于流的生成模型上提出了[NICE: Non-linear Independent Components Estimation](https://arxiv.org/abs/1410.8516)和 [Density estimation using Real NVP](https://arxiv.org/abs/1605.08803)，Glow 正是在这两个模型基础加入可逆 1 x 1 卷积进行扩展，精确的潜在变量推断在人脸属性上展示了惊艳的实验效果，具体效果可在 OpenAI 放出的 Demo([https://blog.openai.com/glow/](https://blog.openai.com/glow/))下查看。

目前已有的生成模型除了 Glow 外包括三大类，**GAN、VAE 和 Autoregressive Model（自回归模型）**。 其中**自回归模型和VAE是基于似然的方法**，**GAN则是通过缩小样本和生成之间的分布实现数据的生成**。

+ **自回归模型（Autoregressive Model）**：自回归模型在**PixelCNN和PixelRNN**上展示了很不错的实验效果，但是由于是**按照像素点去生成图像**导致计算成本高， 在**可并行性上受限**，在处理大型数据如大型图像或视频是具有一定麻烦的。 
+ **变分自编码器（VAE）**：VAE是在Autoencoder的基础上**让图像编码的潜在向量服从高斯分布**从而实现图像的生成，**优化了数据对数似然的下界**，VAE 在图像生成上是**可并行的**， 但是VAE存在着**生成图像模糊**的问题，Glow 文中称之为优化相对具有挑战性。 
+ **生成对抗网络（GAN）**：GAN 的思想就是利用博弈不断的优化生成器和判别器从而使得生成的图像与真实图像在分布上越来越相近。**GAN 生成的图像比较清晰**， 在很多 GAN 的拓展工作中也取得了很大的提高。但是**GAN生成中的多样性不足**以及**训练过程不稳定**是GAN一直以来的问题，同时GAN**没有潜在空间编码器，从而缺乏对数据的全面支持。** 

Glow 在图像的生成，尤其是在**图像编码得到的潜在向量精确推断**上展示了很好的效果。

基于流的生成模型总结一下具有以下优点：

1. **精确的潜在变量推断和对数似然评估**。在 VAE 中编码后**只能推理出对应于数据点的潜在变量的近似值**，GAN 根本就没有编码器更不用谈潜在变量的推断了。在 Glow 这样的**可逆**生成模型中，**可以在没有近似的情况下实现潜在变量的精确的推理**，还可以**优化数据的精确对数似然，而不是其下限。**
2. **高效的推理和合成**。自回归模型如 PixelCNN，也是**可逆**的，然而这样的模型合成难以实现并行化，并且通常在并行硬件上效率低下。而基于流的生成模型如 Glow 和 RealNVP 都能有效实现推理与合成的并行化。
3. **对下游任务有用的潜在空间**。**自回归模型的隐藏层有未知的边际分布**，使其执行有效的数据操作上很困难；在 GAN 中，由于模型没有编码器使得**数据点通常不能在潜在空间中直接被表征**，并且**表征完整的数据分布也是不容易的**。而在可逆生成模型和 VAE 中不会如此，它们**允许多种应用，例如数据点之间的插值，和已有数据点的有目的修改。**
4. **内存的巨大潜力**。如 RevNet 论文所述，在可逆神经网络中**计算梯度需要一定量的内存，而不是线性的深度。**



