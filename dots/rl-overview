// rl-overview
digraph {
	xgjc [label="序贯决策问题" shape=box style=rounded]
	mdp [label="马尔科夫决策过程MDP(s,A,P,R,gamma)" shape=box style=rounded]
	xgjc -> mdp
	model_dp [label="基于模型的动态规划方法" shape=box style=rounded]
	no_model_dp [label="无模型的强化学习方法" shape=box style=rounded]
	mdp -> model_dp [label="(S,A,P,R,gamma)"]
	mdp -> no_model_dp [label="(S,A,P?,R?,gamma?)"]
	policy_iter_model_dp [label="策略迭代" shape=box style=rounded]
	value_iter_model_dp [label="值迭代" shape=box style=rounded]
	policy_search_model_dp [label="策略搜索" shape=box style=rounded]
	policy_iter_no_model_dp [label="策略迭代" shape=box style=rounded]
	value_iter_no_model_dp [label="值迭代" shape=box style=rounded]
	policy_search_no_model_dp [label="策略搜索" shape=box style=rounded]
	value_function_approximation [label="值函数逼近" fillcolor=yellow fontcolor=red shape=box style="rounded,filled"]
	model_dp -> policy_iter_model_dp
	model_dp -> value_iter_model_dp
	model_dp -> policy_search_model_dp
	no_model_dp -> policy_iter_no_model_dp
	no_model_dp -> value_iter_no_model_dp
	no_model_dp -> policy_search_no_model_dp
	value_function_approximation -> policy_iter_model_dp [style=dashed]
	value_function_approximation -> value_iter_model_dp [style=dashed]
	value_function_approximation -> policy_iter_no_model_dp [style=dashed]
	value_function_approximation -> value_iter_no_model_dp [style=dashed]
}
